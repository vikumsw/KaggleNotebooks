{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COMPREHENSIVE GUIDE TO HYPERPARAMETER TUNING\n[Vikum Wijesinghe](https://www.linkedin.com/in/vikumwijesinghe/) - September 2019\n\nOther Kernels: https://www.kaggle.com/vikumsw/kernels\n\n---"},{"metadata":{},"cell_type":"markdown","source":"# Problem Description\n\n## There is an <font color=\"NAVY\">ANGRY BABY</font>. What <font color=\"NAVY\">FLAVOR</font> of <font color=\"NAVY\">ICE CREAM</font> would you think is the best to make the baby pleased?\n\n|||\n|:-:|:-:|\n|![](https://media.giphy.com/media/26gsnlYjswkyY3ENq/giphy.gif)|![](https://media.giphy.com/media/HuAvEGGh9o8rC/giphy.gif)|\n\n## Analogy:\n### * Angry Baby   -> Problem\n### * Ice Cream    -> Choosen ML Algorithm to solve the problem\n### * Flavors      -> Configurations/Properties of the ML Algorithm\n\n\n\n## How are you going to solve the problem?\n### 1. Choose the most popular flavor :-> <font color=\"GRAY\">Using your Machine Learning algorithm with default hyperparameters, You would most likely end up with a suboptimal model.</font>\n### 2. Choose the flavor baby likes the most :-> <font color=\"GRAY\">Tuning your models Hyperparameters to get the most skillful model.</font>\n\n\nKeep Reading to find out how to choose the flavor baby likes the most"},{"metadata":{},"cell_type":"markdown","source":"# Table Of Contents\n\n1. [What are hyperparameters?]()\n1. [Some examples of hyperparameters]()\n1. [Why they are important?]()\n1. [How to tune hyperparamters]()\n1. [Library import & Data Loading]()\n1. [Quick Data Cleaning & Missing Values Handling](#)\n1. [Helper Functions](#)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport sys\nfrom IPython.display import Image\n\ntrain = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv',index_col='Id') \ntest  = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv',index_col='Id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Helper Functions"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def dropDataMissingColumns(df,percentage):\n    print(\"Dropping columns where more than {}% values are Missing..\".format(percentage))\n    nan_percentage = df.isnull().sum().sort_values(ascending=False) / df.shape[0]\n    missing_val = nan_percentage[nan_percentage > 0]\n    to_drop = missing_val[missing_val > percentage/100].index.values\n    df.drop(to_drop, axis=1, inplace=True)\n    \ndef dropTargetMissingRows(df,target):\n    print(\"Dropping Rows where Target is Missing..\")\n    df.dropna(axis=0, subset=[target], inplace=True)\n\ndef fillMissingValues(df):\n    print(\"Filling MissingValues: object cols=\"\"UNKOWN\"\" , Numeric cols=median..\")\n    # for Object columns fill using 'UNKOWN'\n    # for Numeric columns fill using median\n    num_cols = [cname for cname in df.columns if df[cname].dtype in ['int64', 'float64']]\n    cat_cols = [cname for cname in df.columns if df[cname].dtype == \"object\"]\n    values = {}\n    for a in cat_cols:\n        values[a] = 'UNKOWN'\n\n    for a in num_cols:\n        values[a] = df[a].median()\n        \n    df.fillna(value=values,inplace=True)\n\ndef performOneHotEncoding(df,columnsToEncode):\n    return pd.get_dummies(df,columns = columnsToEncode)    \n\ndef getObjectColumnsList(df):\n    return [cname for cname in df.columns if df[cname].dtype == \"object\"]\n\ndef encodeCatFeatures(df,catColsToEncode):\n    print(\"Encoding Categorical Features..\")\n    df = performOneHotEncoding(df,catColsToEncode)\n    return df\n    \ndef quickPreprocessData(df,target,catColsToEncode):\n    print(\"QuickPreprocessData Started..\")\n    dropTargetMissingRows(df,target)\n    fillMissingValues(df)\n    df = encodeCatFeatures(df,catColsToEncode)\n    print(\"QuickPreprocessData Completed\")\n    return df\n\ndef checkDataBeforeTraining(df):\n    if(df.isnull().sum().sum() != 0):\n        print(\"Error : Null Values Exist in Data\")\n        return False;\n    \n    if(len([cname for cname in df.columns if df[cname].dtype == \"object\"])>0):\n        print(\"Error : Object Columns Exist in Data\")\n        return False;\n    \n    print(\"Data is Ready for Training\")\n    return True;\n\ndef getTrainX_TrainY(train,target):\n    trainY = train.loc[:,target]\n    trainX = train.drop(target, axis=1)\n    return trainX,trainY","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Quick Data Cleaning & Missing Values Handling"},{"metadata":{"trusted":true},"cell_type":"code","source":"dropTargetMissingRows(train,'SalePrice')\ntrainX,trainY = getTrainX_TrainY(train,'SalePrice')\ndropDataMissingColumns(trainX,10)\nfillMissingValues(trainX)\ncatColsToEncode = getObjectColumnsList(trainX)\ntrainX = encodeCatFeatures(trainX,catColsToEncode)\n\nif(checkDataBeforeTraining(trainX) == False):\n    sys.exit()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## What are hyperparameters?\n\nLet's create a Random Forest Regressor model for demonstration,"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Image(\"../input/notebookimages/hy.PNG\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These parameters express configurations of the model such as its structure or learning rates. They are called hyperparameters.\nThese values cannot be estimated from data. So hyperparameters are usually set before training. Think of it like there exist different flavors of the same machine learning algorithm.\n\n![](https://media.giphy.com/media/QmKtZGQn4cNi6EC15Y/giphy.gif)\n\n## Some examples of hyperparameters :\n* Number of leaves or depth of a tree\n* Learning rate\n* Number of hidden layers in a deep neural network\n* Number of clusters in a k-means clustering\n\n## Why they are important?.\n\nIn addition to choosing the best suited Machine Learning model for a particular problem, selecting the best falvour of the selected model also decides the performance."},{"metadata":{},"cell_type":"markdown","source":"### Predictive Modelling"},{"metadata":{},"cell_type":"markdown","source":"Now Data is ready for training... First we need a regressor ... lets choose RandomForestRegressor...\nCan't we just call .fit and get the model trainined?. -> yes we can... like below."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import make_regression\nfrom pprint import pprint\n\nX, y = make_regression(n_features=4, n_informative=2,random_state=0, shuffle=False)\nrf = RandomForestRegressor(n_estimators=10)\nrf.fit(X, y) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But is this model tuned to this specific problem? . The answer is no!.\nThis is where hyperparameters come in to play.\n\nHyperparameter tuning is the way we tune a algorithm to a specific dataset to get a better skillful model. \n\n\nLets see the current model."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Parameters currently in use:\\n')\npprint(rf.get_params())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import make_scorer, mean_squared_error\n\ndef mean_squared_error_(ground_truth, predictions):\n    return mean_squared_error(ground_truth, predictions) ** 0.5\n\nRMSE = make_scorer(mean_squared_error_, greater_is_better=False)\n\nrfr = RandomForestRegressor(n_jobs=1, random_state=0)\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [50, 100],\n    'max_features': ['auto'],\n    'min_samples_leaf': [2],\n    'min_samples_split': [5],\n    'n_estimators': [200, 2000]}\n'''\nmodel = GridSearchCV(estimator=rfr, param_grid=param_grid, n_jobs=-1, cv=10, scoring=RMSE, verbose=2)\nmodel.fit(trainX, trainY)\nscore = -model.best_score_\n\nprint('Random forecast regression...')\nprint(rfr)\nprint('Best Params:\\n', model.best_params_)\nprint('Best CV Score:', score)\n\n#y_pred = model.predict(X_test)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import make_regression\nimport seaborn as sns; sns.set()\n#import matplotlib.pyplot as plt\n\nX, y = make_regression(n_features=4, n_informative=2,random_state=0, shuffle=False)\n\nX.shape\ny.shape\n\nax = sns.scatterplot(x=X[:,0], y=y)\nax = sns.scatterplot(x=X[:,1], y=y)\nax = sns.scatterplot(x=X[:,2], y=y)\nax = sns.scatterplot(x=X[:,3], y=y)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}